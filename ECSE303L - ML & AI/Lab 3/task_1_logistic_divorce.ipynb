{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMHjtVPbyaKP"
   },
   "source": [
    "## Logistic Regression Model for Divorce Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1: Implement  linear regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJi26z8awmSD"
   },
   "source": [
    "### Logistic regression\n",
    "Logistic regression uses an equation as the representation, very much like linear regression.\n",
    "\n",
    "Input values (x) are combined linearly using weights or coefficient values (referred to as W) to predict an output value (y). A key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a continuous value.<br>\n",
    "\n",
    "###  $\\hat{y}(w, x) = \\frac{1}{1+exp^{-(w_0 + w_1 * x_1 + ... + w_p * x_p)}}$\n",
    "\n",
    "#### Dataset\n",
    "The dataset is available at <strong>\"data/divorce.csv\"</strong> in the respective challenge's repo.<br>\n",
    "<strong>Original Source:</strong> https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set. Dataset is based on rating for questionnaire filled by people who already got divorse and those who is happily married.<br><br>\n",
    "\n",
    "[//]: # \"The dataset is available at http://archive.ics.uci.edu/ml/machine-learning-databases/00520/data.zip. Unzip the file and use either CSV or xlsx file.<br>\"\n",
    "\n",
    "\n",
    "#### Features (X)\n",
    "1. Atr1 - If one of us apologizes when our discussion deteriorates, the discussion ends. (Numeric | Range: 0-4)\n",
    "2. Atr2 - I know we can ignore our differences, even if things get hard sometimes. (Numeric | Range: 0-4)\n",
    "3. Atr3 - When we need it, we can take our discussions with my spouse from the beginning and correct it. (Numeric | Range: 0-4)\n",
    "4. Atr4 - When I discuss with my spouse, to contact him will eventually work. (Numeric | Range: 0-4)\n",
    "5. Atr5 - The time I spent with my wife is special for us. (Numeric | Range: 0-4)\n",
    "6. Atr6 - We don't have time at home as partners. (Numeric | Range: 0-4)\n",
    "7. Atr7 - We are like two strangers who share the same environment at home rather than family. (Numeric | Range: 0-4)\n",
    "\n",
    "&emsp;.<br>\n",
    "&emsp;.<br>\n",
    "&emsp;.<br>\n",
    "<br>\n",
    "54. Atr54 - I'm not afraid to tell my spouse about her/his incompetence. (Numeric | Range: 0-4)\n",
    "<br><br>\n",
    "Take a look above at the source of the original dataset for more details.\n",
    "\n",
    "#### Target (y)\n",
    "55. Class: (Binary | 1 => Divorced, 0 => Not divorced yet)\n",
    "\n",
    "#### Objective\n",
    "To gain understanding of logistic regression through implementing the model from scratch\n",
    "\n",
    "#### Tasks\n",
    "- Download and load the data (csv file contains ';' as delimiter)\n",
    "- Add column at position 0 with all values=1 (pandas.DataFrame.insert function). This is for input to the bias $w_0$\n",
    "- Define X matrix (independent features) and y vector (target feature) as numpy arrays\n",
    "- Print the shape and datatype of both X and y\n",
    "[//]: # \"- Dataset contains missing values, hence fill the missing values (NA) by performing missing value prediction\"\n",
    "[//]: # \"- Since the all the features are in higher range, columns can be normalized into smaller scale (like 0 to 1) using different methods such as scaling, standardizing or any other suitable preprocessing technique (sklearn.preprocessing.StandardScaler)\"\n",
    "- Split the dataset into 85% for training and rest 15% for testing (sklearn.model_selection.train_test_split function)\n",
    "- Follow logistic regression class and fill code where highlighted:\n",
    "    - Write sigmoid function to predict probabilities\n",
    "    - Write log likelihood function\n",
    "    - Write fit function where gradient ascent is implemented\n",
    "    - Write predict_proba function where we predict probabilities for input data\n",
    "- Train the model\n",
    "- Write function for calculating accuracy\n",
    "- Compute accuracy on train and test data\n",
    "\n",
    "#### Further Fun (will not be evaluated)\n",
    "- Play with learning rate and max_iterations\n",
    "- Preprocess data with different feature scaling methods (i.e. scaling, normalization, standardization, etc) and observe accuracies on both X_train and X_test\n",
    "- Train model on different train-test splits such as 60-40, 50-50, 70-30, 80-20, 90-10, 95-5 etc. and observe accuracies on both X_train and X_test\n",
    "- Shuffle training samples with different random seed values in the train_test_split function. Check the model error for the testing data for each setup.\n",
    "- Print other classification metrics such as:\n",
    "    - classification report (sklearn.metrics.classification_report),\n",
    "    - confusion matrix (sklearn.metrics.confusion_matrix),\n",
    "    - precision, recall and f1 scores (sklearn.metrics.precision_recall_fscore_support)\n",
    "\n",
    "#### Helpful links\n",
    "- How Logistic Regression works: https://machinelearningmastery.com/logistic-regression-for-machine-learning/\n",
    "- Feature Scaling: https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "- Training testing splitting: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "- Use slack for doubts: https://join.slack.com/t/deepconnectai/shared_invite/zt-givlfnf6-~cn3SQ43k0BGDrG9_YOn4g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21J6cpd_wmSE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4SL1fdNt1k3Q"
   },
   "outputs": [],
   "source": [
    "# Download the dataset from the source\n",
    "#Dataset loaded locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9av7W-wowmSI"
   },
   "outputs": [],
   "source": [
    "# Read the data from local cloud directory\n",
    "data = pd.read_csv('./data/divorce.csv', sep=';')\n",
    "# Set delimiter to semicolon(;) in case of unexpected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column which has all 1s\n",
    "# The idea is that weight corresponding to this column is equal to intercept\n",
    "# This way it is efficient and easier to handle the bias/intercept term\n",
    "data.insert(0, \"Atr0\", [1]*len(data), allow_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eV1jGAQxwmSP"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Atr0</th>\n",
       "      <th>Atr1</th>\n",
       "      <th>Atr2</th>\n",
       "      <th>Atr3</th>\n",
       "      <th>Atr4</th>\n",
       "      <th>Atr5</th>\n",
       "      <th>Atr6</th>\n",
       "      <th>Atr7</th>\n",
       "      <th>Atr8</th>\n",
       "      <th>Atr9</th>\n",
       "      <th>...</th>\n",
       "      <th>Atr46</th>\n",
       "      <th>Atr47</th>\n",
       "      <th>Atr48</th>\n",
       "      <th>Atr49</th>\n",
       "      <th>Atr50</th>\n",
       "      <th>Atr51</th>\n",
       "      <th>Atr52</th>\n",
       "      <th>Atr53</th>\n",
       "      <th>Atr54</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Atr0  Atr1  Atr2  Atr3  Atr4  Atr5  Atr6  Atr7  Atr8  Atr9  ...  Atr46  \\\n",
       "0     1     2     2     4     1     0     0     0     0     0  ...      2   \n",
       "1     1     4     4     4     4     4     0     0     4     4  ...      2   \n",
       "2     1     2     2     2     2     1     3     2     1     1  ...      3   \n",
       "3     1     3     2     3     2     3     3     3     3     3  ...      2   \n",
       "4     1     2     2     1     1     1     1     0     0     0  ...      2   \n",
       "5     1     0     0     1     0     0     2     0     0     0  ...      2   \n",
       "6     1     3     3     3     2     1     3     4     3     2  ...      3   \n",
       "7     1     2     1     2     2     2     1     0     3     3  ...      0   \n",
       "8     1     2     2     1     0     0     4     1     3     3  ...      1   \n",
       "9     1     1     1     1     1     1     2     0     2     2  ...      2   \n",
       "\n",
       "   Atr47  Atr48  Atr49  Atr50  Atr51  Atr52  Atr53  Atr54  Class  \n",
       "0      1      3      3      3      2      3      2      1      1  \n",
       "1      2      3      4      4      4      4      2      2      1  \n",
       "2      2      3      1      1      1      2      2      2      1  \n",
       "3      2      3      3      3      3      2      2      2      1  \n",
       "4      1      2      3      2      2      2      1      0      1  \n",
       "5      2      1      2      1      1      1      2      0      1  \n",
       "6      2      3      2      3      3      2      2      2      1  \n",
       "7      1      2      2      2      1      1      1      0      1  \n",
       "8      1      1      1      1      1      1      1      1      1  \n",
       "9      0      2      2      2      2      4      3      3      1  \n",
       "\n",
       "[10 rows x 56 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the dataframe rows just to see some samples\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "joRU6dWxwmSR"
   },
   "outputs": [],
   "source": [
    "# Define X (input features) and y (output feature) \n",
    "X = data.iloc[:,0:55].values\n",
    "y = np.array(data.Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAyM-CYCwmSU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: Type-<class 'numpy.ndarray'>, Shape-(170, 55)\n",
      "y: Type-<class 'numpy.ndarray'>, Shape-(170,)\n"
     ]
    }
   ],
   "source": [
    "X_shape = X.shape\n",
    "X_type  = type(X)\n",
    "y_shape = y.shape\n",
    "y_type  = type(y)\n",
    "print(f'X: Type-{X_type}, Shape-{X_shape}')\n",
    "print(f'y: Type-{y_type}, Shape-{y_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Expected output: </strong><br><br>\n",
    "\n",
    "X: Type-<class 'numpy.ndarray'>, Shape-(170, 55)<br>\n",
    "y: Type-<class 'numpy.ndarray'>, Shape-(170,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdLIVOm127-z"
   },
   "outputs": [],
   "source": [
    "# Check and fill any missing values if any\n",
    "for v in data.isnull().sum():\n",
    "    if v != 0:\n",
    "        print('Null present')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "En9Kb9dh2-wm"
   },
   "outputs": [],
   "source": [
    "# Perform standarization (if required)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "features = list(data.iloc[:,1:55].columns)\n",
    "scalar = StandardScaler()\n",
    "# the fit_transform ops returns a 2d numpy.array, we cast it to a pd.DataFrame\n",
    "standardized_features = pd.DataFrame(scalar.fit_transform(data[features].copy()), columns = features)\n",
    "old_shape = data.shape\n",
    "# drop the unnormalized features from the dataframe\n",
    "data.drop(features, axis = 1, inplace = True)\n",
    "# join back the normalized features\n",
    "data = pd.concat([data, standardized_features], axis= 1)\n",
    "assert old_shape == data.shape, \"something went wrong!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8WF-EqO3BEa"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing here\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "acCATJhI3FdH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (144, 55) , y_train: (144,)\n",
      "X_test: (26, 55) , y_test: (26,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of features and target of training and testing: X_train, X_test, y_train, y_test\n",
    "X_train_shape = X_train.shape\n",
    "y_train_shape = y_train.shape\n",
    "X_test_shape  = X_test.shape\n",
    "y_test_shape  = y_test.shape\n",
    "\n",
    "print(f\"X_train: {X_train_shape} , y_train: {y_train_shape}\")\n",
    "print(f\"X_test: {X_test_shape} , y_test: {y_test_shape}\")\n",
    "assert (X_train.shape[0]==y_train.shape[0] and X_test.shape[0]==y_test.shape[0]), \"Check your splitting carefully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSa7cW-NwmSd"
   },
   "source": [
    "##### Let us start implementing logistic regression from scratch. Just follow code cells, see hints if required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We will build a LogisticRegression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT ANY VARIABLE OR FUNCTION NAME(S) IN THIS CELL\n",
    "# Let's try more object oriented approach this time :)\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        '''Initialize variables\n",
    "        Args:\n",
    "            learning_rate  : Learning Rate\n",
    "            max_iterations : Max iterations for training weights\n",
    "        '''\n",
    "        # Initialising all the parameters\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.likelihoods    = []\n",
    "        \n",
    "        # Define epsilon because log(0) is not defined\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        '''Sigmoid function: f:R->(0,1)\n",
    "        Args:\n",
    "            z : A numpy array (num_samples,)\n",
    "        Returns:\n",
    "            A numpy array where sigmoid function applied to every element\n",
    "        '''\n",
    "        ### START CODE HERE\n",
    "        sig_z = 1 / (1 + np.exp(-z))\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        assert (z.shape==sig_z.shape), 'Error in sigmoid implementation. Check carefully'\n",
    "        return sig_z\n",
    "    \n",
    "    def log_likelihood(self, y_true, y_pred):\n",
    "        '''Calculates maximum likelihood estimate\n",
    "        Remember: y * log(yh) + (1-y) * log(1-yh)\n",
    "        Note: Likelihood is defined for multiple classes as well, but for this dataset\n",
    "        we only need to worry about binary/bernoulli likelihood function\n",
    "        Args:\n",
    "            y_true : Numpy array of actual truth values (num_samples,)\n",
    "            y_pred : Numpy array of predicted values (num_samples,)\n",
    "        Returns:\n",
    "            Log-likelihood, scalar value\n",
    "        '''\n",
    "        # Fix 0/1 values in y_pred so that log is not undefined\n",
    "        y_pred = np.maximum(np.full(y_pred.shape, self.eps), np.minimum(np.full(y_pred.shape, 1-self.eps), y_pred))\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        likelihood =  np.sum(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return likelihood\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''Trains logistic regression model using gradient ascent\n",
    "        to gain maximum likelihood on the training data\n",
    "        Args:\n",
    "            X : Numpy array (num_examples, num_features)\n",
    "            y : Numpy array (num_examples, )\n",
    "        Returns: VOID\n",
    "        '''\n",
    "        \n",
    "        num_examples = X.shape[0]\n",
    "        num_features = X.shape[1]\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        \n",
    "        # Initialize weights with appropriate shape\n",
    "        self.weights = np.random.rand(num_features)\n",
    "        \n",
    "        # Perform gradient ascent\n",
    "        for i in range(self.max_iterations):\n",
    "            # Define the linear hypothesis(z) first\n",
    "            # HINT: what is our hypothesis function in linear regression, remember?\n",
    "            z = X@self.weights\n",
    "            \n",
    "            # Output probability value by appplying sigmoid on z\n",
    "            y_pred = self.sigmoid(z)\n",
    "            \n",
    "            # Calculate the gradient values\n",
    "            # This is just vectorized efficient way of implementing gradient. Don't worry, we will discuss it later.\n",
    "            gradient = np.mean((y-y_pred)*X.T, axis=1)\n",
    "            \n",
    "            # Update the weights\n",
    "            # Caution: It is gradient ASCENT not descent\n",
    "            self.weights = self.weights + gradient\n",
    "            \n",
    "            # Calculating log likelihood\n",
    "            likelihood = self.log_likelihood(y, y_pred)\n",
    "\n",
    "            self.likelihoods.append(likelihood)\n",
    "    \n",
    "        ### END CODE HERE\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        '''Predict probabilities for given X.\n",
    "        Remember sigmoid returns value between 0 and 1.\n",
    "        Args:\n",
    "            X : Numpy array (num_samples, num_features)\n",
    "        Returns:\n",
    "            probabilities: Numpy array (num_samples,)\n",
    "        '''\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"Fit the model before prediction\")\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        z = X@self.weights\n",
    "        probabilities = self.sigmoid(z)\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        '''Predict/Classify X in classes\n",
    "        Args:\n",
    "            X         : Numpy array (num_samples, num_features)\n",
    "            threshold : scalar value above which prediction is 1 else 0\n",
    "        Returns:\n",
    "            binary_predictions : Numpy array (num_samples,)\n",
    "        '''\n",
    "        # Thresholding probability to predict binary values\n",
    "        binary_predictions = np.array(pd.DataFrame(self.predict_proba(X)).applymap(lambda x: 1 if x>threshold else 0))\n",
    "        \n",
    "        return binary_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now initialize logitic regression implemented by you\n",
    "model = MyLogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now fit on training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phew!! That's a lot of code. But you did it, congrats !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tvMc0OqwmSp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood on training data: -0.08797981505787177\n"
     ]
    }
   ],
   "source": [
    "# Train log-likelihood\n",
    "train_log_likelihood = model.log_likelihood(y_train, model.predict_proba(X_train))\n",
    "print(\"Log-likelihood on training data:\", train_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZQ8ITUt4b0N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood on testing data: -10.571108264063914\n"
     ]
    }
   ],
   "source": [
    "# Test log-likelihood\n",
    "test_log_likelihood = model.log_likelihood(y_test, model.predict_proba(X_test))\n",
    "print(\"Log-likelihood on testing data:\", test_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot([i+1 for i in range(len(model.likelihoods))], model.likelihoods)\n",
    "plt.title(\"Log-Likelihood curve\")\n",
    "plt.xlabel(\"Iteration num\")\n",
    "plt.ylabel(\"Log-likelihood\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's calculate accuracy as well. Accuracy is defined simply as the rate of correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on test data\n",
    "y_pred = model.predict(X_test)\n",
    "n1 = np.array([1,2,3,4,2])\n",
    "n2 = pd.DataFrame(np.array([1,2,2,3,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true,y_pred):\n",
    "    '''Compute accuracy.\n",
    "    Accuracy = (Correct prediction / number of samples)\n",
    "    Args:\n",
    "        y_true : Truth binary values (num_examples, )\n",
    "        y_pred : Predicted binary values (num_examples, )\n",
    "    Returns:\n",
    "        accuracy: scalar value\n",
    "    '''\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    n_sample = y_true.shape[0]\n",
    "    c_pred = 0\n",
    "    for i in range(n_sample):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            c_pred+=1\n",
    "    accuracy = c_pred/n_sample\n",
    "    ### END CODE HERE\n",
    "    return accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.31%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy on train data\n",
    "print('{:.2f}%'.format(accuracy(y_test, (model.predict(X_train)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.31%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy on test data\n",
    "print('{:.2f}%'.format(accuracy(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: Use Logistic Regression from sklearn on the same dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "- Define X and y again for sklearn Linear Regression model\n",
    "- Train Logistic Regression Model on the training set (sklearn.linear_model.LogisticRegression class)\n",
    "- Run the model on testing set\n",
    "- Print 'accuracy' obtained on the testing dataset (sklearn.metrics.accuracy_score function)\n",
    "\n",
    "#### Further fun (will not be evaluated)\n",
    "- Compare accuracies of your model and sklearn's logistic regression model\n",
    "\n",
    "#### Helpful links\n",
    "- Classification metrics in sklearn: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model from sklearn\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on testing set X_test\n",
    "y_pred = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Accuracy on testing set\n",
    "test_accuracy_sklearn = \n",
    "\n",
    "print(f\"\\nAccuracy on testing set: {test_accuracy_sklearn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task_1_logistic_divorse.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
